from pathlib import Path
import streamlit as st
from Menu import menu
import pickle
from Model import *
from sklearn.metrics import confusion_matrix, precision_score, recall_score, fbeta_score
import plotly.express as px
import streamlit_shadcn_ui as ui

st.set_page_config(page_title='Ocena modelu', page_icon='', layout='wide')

menu()

st.title('Ocena modelu')
st.markdown('Dokonaj oceny jakoci modelu na zbiorze testowym korzystajc z popularnych metryk.')

cols = st.columns(2, gap='large')

with cols[0]:
    st.subheader("Ocena og贸lna")
    st.write("Popularn metryk oceny jakoci klasyfikatora jest dokadno, kt贸ra informuje o tym jaka cz wszystkich predykcji modelu jest poprawna.")

    with open(str(Path(__file__).parents[1]) + '/Dataset/dataset_for_model.pickle', 'rb') as file:
        dataset = pickle.load(file, encoding='latin1')

    with open(str(Path(__file__).parents[1]) + '/Model/evaluation_results.pickle', 'rb') as file2:
        evaluation_results = pickle.load(file2, encoding='latin1')

    labels = ['Rakieta', 'Czog', 'Pocig', 'Samolot', 'Statek']

    ui.metric_card("Dokadno", content=str(round(evaluation_results['acc'] * 100, 2)) + '%')

    cm = confusion_matrix(dataset['y_test'], evaluation_results['y_pred'])

    fig = px.imshow(cm.tolist(), text_auto=True, x=labels, y=labels, width=750, height=750,
                    labels=dict(x="Klasa prawdziwa", y="Klasa przewidywana", color='Liczno'),
                    title='Macierz pomyek')
    # fig.write_image(str(Path(__file__).parents[1]) + '/pages/Model_evaluation/Confusion_matrix.jpg')
    st.plotly_chart(fig)

    with st.expander("Wnioski", expanded=False):
        st.write("Pod wzgldem iloci poprawnych klasyfikacji, model najlepiej radzi sobie z obrazami czog贸w, natomiast najgorzej z obrazami pocig贸w. "
                 "Najczciej popenianym bdem jest niepoprawna klasyfikacja czogu jako pocig. "
                 "Cieszy natomiast fakt niskiego odsetka pomyek klas pod wzgldem rodzaju trasportu (ldowy, powietrzny oraz morski). "
                 )

with cols[1]:

    st.subheader("Ocena dla poszczeg贸lnych klas")

    st.write("Analizujc konkretn klas model traktowa mo偶na jako klasyfikator binarny zgodnie z podejciem jeden kontra reszta, "
             "w kt贸rym wybran klas traktuje si jako pozytywn, a wszystkie pozostae jako negatywne. Wynik predykcji klasyfikatora binarnego "
             "zakwalifikowa mo偶na do jednej z 4 kategorii: ")

    st.markdown("""
    - Prawdziwie pozytywny (TP) - model sklasyfikowa przykad pochodzcy z klasy pozytywnej jako klasa pozytywn,
    - Prawdziwie negatywny (TN) - model sklasyfikowa przykad pochodzcy z klasy negatywnej jako klas negatywn,
    - Faszywie pozytywny (FP) - model sklasyfikowal przykad pochodzcy z klasy pozytywnej jako klas negatywn (bd pierwszego rodzaju),
    - Faszywie negatywny (FN) - model sklasyfikowal przykad pochodzcy z klasy negatywnej jako klas pozytywn (bd drugiego rodzaju).
    """)

    precision = precision_score(dataset['y_test'], evaluation_results['y_pred'], average=None)
    recall = recall_score(dataset['y_test'], evaluation_results['y_pred'], average=None)
    f_1 = fbeta_score(dataset['y_test'], evaluation_results['y_pred'], beta=1, average=None)

    st.write(
        "W zale偶noci od zastosowa modelu, w ocenie klasyfikatora binarnego skorzysta mo偶na z nastpujcych metryk:")

    cols2 = st.columns(3)

    with cols2[0]:
        st.markdown("Precyzja zdefiniowana jako")
        st.latex(r'\frac{TP}{TP + FP}')

    with cols2[1]:
        st.write("Czuo zdefiniowana jako")
        st.latex(r'\frac{TP}{TP + FN}')

    with cols2[2]:
        st.write("F1 jako rednia harmoniczna precyzji oraz czuoci")
        st.latex(r'\frac{2TP}{2TP + FP + FN}')


    st.divider()

    selected_class = st.selectbox('Wybierz klas aby pozna warto wymienionych metryk', options=labels, index=None,
                                  placeholder='Wybierz klas')

    cols3 = st.columns(3)

    if selected_class is not None:
        with cols3[0]:
            ui.metric_card("Precyzja", content=str(round(precision[labels.index(selected_class)] * 100, 1)) + '%')
        with cols3[1]:
            ui.metric_card("Czuo", content=str(round(recall[labels.index(selected_class)] * 100, 1)) + '%')
        with cols3[2]:
            ui.metric_card("F1", content=str(round(f_1[labels.index(selected_class)] * 100, 1)) + '%')


    with st.expander("O metryce F1"):
        st.write("Metryka F1 czc w sobie precyzj oraz czuo umo偶liwia trafn ocen modeli w "
                 "przypadku niezbilanowanych klas jak r贸wnie偶 wykrywa sytuacje, w kt贸rych model ignoruje jeden z typ贸w bd贸w na rzecz poprawy drugiego. ")





